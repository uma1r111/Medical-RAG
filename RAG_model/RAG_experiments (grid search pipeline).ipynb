{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuasrPpudopw",
        "outputId": "11bb7837-cc6e-4d29-d273-c39cf6da768f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install required packages\n",
        "!pip install -q langchain langchain-community pymupdf faiss-cpu rank_bm25 transformers \\\n",
        "    sentence-transformers accelerate tqdm nltk rouge-score summarizers scikit-learn \\\n",
        "    pandas seaborn matplotlib\n",
        "# Write your code to app.py\n",
        "\n",
        "import faiss\n",
        "import json\n",
        "import torch\n",
        "# Memory optimization imports\n",
        "import gc\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "#%%\n",
        "### 1. Imports & Configuration\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import json\n",
        "import nltk\n",
        "from typing import List, Dict, Tuple, Union, Optional, Any\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from langchain.schema import Document\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM, pipeline as hf_pipeline\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from rouge_score import rouge_scorer\n",
        "import textwrap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import random\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')  # Often useful for text processing\n",
        "nltk.download('wordnet')   # For lemmatization if needed\n",
        "# Check GPU availability and set device\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if device.type == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Define global constants\n",
        "MAX_SUMMARY_LENGTH = 512\n",
        "CACHE_DIR = \"./model_cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "#%%\n",
        "### 2. Data Preprocessing & Preparation\n",
        "class DataProcessor:\n",
        "    \"\"\"Handle data loading, preprocessing, and chunking with different strategies\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_and_chunk(file_paths: Union[str, List[str]],\n",
        "                       chunking_strategy: str = \"recursive\",\n",
        "                       chunk_size: int = 512,\n",
        "                       chunk_overlap: int = 64,\n",
        "                       force_reload: bool = False) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Process one or more PDFs into chunks with metadata using specified strategy\n",
        "\n",
        "        Args:\n",
        "            file_paths: Path to the PDF file or list of PDF file paths\n",
        "            chunking_strategy: Strategy for chunking ('recursive', 'token', 'sentence', 'paragraph')\n",
        "            chunk_size: Size of chunks\n",
        "            chunk_overlap: Overlap between chunks\n",
        "            force_reload: Whether to force reload from PDF instead of using cache\n",
        "\n",
        "        Returns:\n",
        "            List of Document objects\n",
        "        \"\"\"\n",
        "        # Handle single file path as a list for consistency\n",
        "        if isinstance(file_paths, str):\n",
        "            file_paths = [file_paths]\n",
        "\n",
        "        # Generate a unique cache key based on all file paths\n",
        "        cache_key = \"_\".join([os.path.basename(fp) for fp in file_paths])\n",
        "        cache_path = f\"./cache_{cache_key}_{chunking_strategy}_{chunk_size}_{chunk_overlap}.json\"\n",
        "\n",
        "        # Try to load from cache if available\n",
        "        if os.path.exists(cache_path) and not force_reload:\n",
        "            print(f\"Loading chunks from cache: {cache_path}\")\n",
        "            try:\n",
        "                with open(cache_path, 'r', encoding='utf-8') as f:\n",
        "                    cached_data = json.load(f)\n",
        "\n",
        "                chunks = []\n",
        "                for item in cached_data:\n",
        "                    chunks.append(Document(\n",
        "                        page_content=item['page_content'],\n",
        "                        metadata=item['metadata']\n",
        "                    ))\n",
        "                print(f\"Loaded {len(chunks)} chunks from cache\")\n",
        "                return chunks\n",
        "            except (json.JSONDecodeError, KeyError) as e:\n",
        "                print(f\"Error loading cache file {cache_path}: {str(e)}. Regenerating chunks...\")\n",
        "\n",
        "        # Process each PDF and combine chunks\n",
        "        all_chunks = []\n",
        "        for file_path in file_paths:\n",
        "            print(f\"\\nProcessing file {file_path} with {chunking_strategy} strategy, chunk_size={chunk_size}, overlap={chunk_overlap}\")\n",
        "            loader = PyMuPDFLoader(file_path)\n",
        "            pages = loader.load()\n",
        "            print(f\"Loaded {len(pages)} pages from {file_path}\")\n",
        "\n",
        "            # Clean text (remove excessive whitespace, etc.)\n",
        "            for page in pages:\n",
        "                page.page_content = DataProcessor._clean_text(page.page_content)\n",
        "                # Add source file metadata\n",
        "                page.metadata['source_file'] = os.path.basename(file_path)\n",
        "\n",
        "            # Apply chunking strategy\n",
        "            if chunking_strategy == \"recursive\":\n",
        "                splitter = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size=chunk_size,\n",
        "                    chunk_overlap=chunk_overlap,\n",
        "                    length_function=len,\n",
        "                    add_start_index=True\n",
        "                )\n",
        "                chunks = splitter.split_documents(pages)\n",
        "\n",
        "            elif chunking_strategy == \"token\":\n",
        "                splitter = TokenTextSplitter(\n",
        "                    chunk_size=chunk_size,\n",
        "                    chunk_overlap=chunk_overlap\n",
        "                )\n",
        "                chunks = splitter.split_documents(pages)\n",
        "\n",
        "            elif chunking_strategy == \"sentence\":\n",
        "                chunks = DataProcessor._sentence_based_chunking(pages, chunk_size, chunk_overlap)\n",
        "\n",
        "            elif chunking_strategy == \"paragraph\":\n",
        "                chunks = DataProcessor._paragraph_based_chunking(pages, chunk_size, chunk_overlap)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown chunking strategy: {chunking_strategy}\")\n",
        "\n",
        "            print(f\"Created {len(chunks)} chunks from {file_path}\")\n",
        "            all_chunks.extend(chunks)\n",
        "\n",
        "        # Cache the combined results\n",
        "        cache_data = []\n",
        "        for chunk in all_chunks:\n",
        "            cache_data.append({\n",
        "                'page_content': chunk.page_content,\n",
        "                'metadata': chunk.metadata\n",
        "            })\n",
        "\n",
        "        try:\n",
        "            with open(cache_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(cache_data, f, ensure_ascii=False, indent=2)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not save cache file {cache_path}: {str(e)}\")\n",
        "\n",
        "        return all_chunks\n",
        "    @staticmethod\n",
        "    def _clean_text(text: str) -> str:\n",
        "        \"\"\"Clean text by removing excessive whitespace and normalizing\"\"\"\n",
        "        # Replace multiple newlines with single newline\n",
        "        text = re.sub(r'\\n+', '\\n', text)\n",
        "        # Replace multiple spaces with single space\n",
        "        text = re.sub(r' +', ' ', text)\n",
        "        # Strip whitespace\n",
        "        text = text.strip()\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def _sentence_based_chunking(pages: List[Document], chunk_size: int, chunk_overlap: int) -> List[Document]:\n",
        "        \"\"\"Create chunks based on sentence boundaries\"\"\"\n",
        "        chunks = []\n",
        "        for page in pages:\n",
        "            sentences = sent_tokenize(page.page_content)\n",
        "            current_chunk = []\n",
        "            current_size = 0\n",
        "\n",
        "            for sentence in sentences:\n",
        "                sentence_size = len(sentence)\n",
        "\n",
        "                if current_size + sentence_size > chunk_size and current_chunk:\n",
        "                    # Create chunk from current sentences\n",
        "                    chunk_text = \" \".join(current_chunk)\n",
        "                    chunks.append(Document(\n",
        "                        page_content=chunk_text,\n",
        "                        metadata={**page.metadata, 'chunk_strategy': 'sentence'}\n",
        "                    ))\n",
        "\n",
        "                    # Handle overlap by keeping some sentences\n",
        "                    overlap_size = 0\n",
        "                    overlap_chunk = []\n",
        "                    for s in reversed(current_chunk):\n",
        "                        if overlap_size + len(s) <= chunk_overlap:\n",
        "                            overlap_chunk.insert(0, s)\n",
        "                            overlap_size += len(s)\n",
        "                        else:\n",
        "                            break\n",
        "\n",
        "                    current_chunk = overlap_chunk\n",
        "                    current_size = overlap_size\n",
        "\n",
        "                current_chunk.append(sentence)\n",
        "                current_size += sentence_size\n",
        "\n",
        "            # Add remaining text as final chunk\n",
        "            if current_chunk:\n",
        "                chunk_text = \" \".join(current_chunk)\n",
        "                chunks.append(Document(\n",
        "                    page_content=chunk_text,\n",
        "                    metadata={**page.metadata, 'chunk_strategy': 'sentence'}\n",
        "                ))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    @staticmethod\n",
        "    def _paragraph_based_chunking(pages: List[Document], chunk_size: int, chunk_overlap: int) -> List[Document]:\n",
        "        \"\"\"Create chunks based on paragraph boundaries\"\"\"\n",
        "        chunks = []\n",
        "        for page in pages:\n",
        "            # Split by double newline to identify paragraphs\n",
        "            paragraphs = re.split(r'\\n\\s*\\n', page.page_content)\n",
        "            paragraphs = [p for p in paragraphs if p.strip()]  # Remove empty paragraphs\n",
        "\n",
        "            current_chunk = []\n",
        "            current_size = 0\n",
        "\n",
        "            for paragraph in paragraphs:\n",
        "                paragraph_size = len(paragraph)\n",
        "\n",
        "                if current_size + paragraph_size > chunk_size and current_chunk:\n",
        "                    # Create chunk from current paragraphs\n",
        "                    chunk_text = \"\\n\\n\".join(current_chunk)\n",
        "                    chunks.append(Document(\n",
        "                        page_content=chunk_text,\n",
        "                        metadata={**page.metadata, 'chunk_strategy': 'paragraph'}\n",
        "                    ))\n",
        "\n",
        "                    # Handle overlap by keeping some paragraphs\n",
        "                    overlap_size = 0\n",
        "                    overlap_chunk = []\n",
        "                    for p in reversed(current_chunk):\n",
        "                        if overlap_size + len(p) <= chunk_overlap:\n",
        "                            overlap_chunk.insert(0, p)\n",
        "                            overlap_size += len(p)\n",
        "                        else:\n",
        "                            break\n",
        "\n",
        "                    current_chunk = overlap_chunk\n",
        "                    current_size = overlap_size\n",
        "\n",
        "                current_chunk.append(paragraph)\n",
        "                current_size += paragraph_size\n",
        "\n",
        "            # Add remaining text as final chunk\n",
        "            if current_chunk:\n",
        "                chunk_text = \"\\n\\n\".join(current_chunk)\n",
        "                chunks.append(Document(\n",
        "                    page_content=chunk_text,\n",
        "                    metadata={**page.metadata, 'chunk_strategy': 'paragraph'}\n",
        "                ))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess_for_query(text: str) -> str:\n",
        "        \"\"\"Preprocess query text\"\"\"\n",
        "        return text.strip()\n",
        "\n",
        "#%%\n",
        "### 3. Advanced Retrieval System\n",
        "class AdvancedRetriever:\n",
        "    \"\"\"\n",
        "    Enhanced retrieval system with multiple retrieval methods:\n",
        "    - Dense retrieval (embedding-based)\n",
        "    - Sparse retrieval (BM25)\n",
        "    - Hybrid retrieval (combining dense and sparse)\n",
        "    - Re-ranking with cross-encoders\n",
        "    - Contextual embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chunks: List[Document], embedding_model: str = \"BAAI/bge-small-en\"):\n",
        "        self.chunks = chunks\n",
        "        print(f\"Loading dense embedding model: {embedding_model}...\")\n",
        "        self.dense_model = SentenceTransformer(embedding_model, cache_folder=CACHE_DIR, device=str(device))\n",
        "\n",
        "        # Load cross-encoder for re-ranking\n",
        "        print(\"Loading cross-encoder for re-ranking...\")\n",
        "        self.cross_encoder = CrossEncoder(\n",
        "            'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
        "            cache_dir=CACHE_DIR,\n",
        "            device=str(device)\n",
        "        )\n",
        "\n",
        "        self._create_indices()\n",
        "        print(f\"Initialized retriever with {len(self.chunks)} documents\")\n",
        "\n",
        "    def _create_indices(self):\n",
        "        \"\"\"Create all necessary indices for retrieval\"\"\"\n",
        "        # Extract texts and prepare for indexing\n",
        "        texts = [chunk.page_content for chunk in self.chunks]\n",
        "\n",
        "        # Dense Index\n",
        "        print(\"Creating dense embeddings...\")\n",
        "        self.embeddings = self.dense_model.encode(texts, show_progress_bar=True, device=str(device))\n",
        "\n",
        "        # Use GPU-accelerated FAISS if available\n",
        "        res = faiss.StandardGpuResources() if faiss.get_num_gpus() > 0 else None\n",
        "\n",
        "        # Create indices\n",
        "        print(\"Creating FAISS index...\")\n",
        "        dimension = self.embeddings.shape[1]\n",
        "\n",
        "        # Create both L2 (Euclidean) and Inner Product (cosine) indices\n",
        "        self.index_ip = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
        "        self.index_l2 = faiss.IndexFlatL2(dimension)  # L2 distance\n",
        "\n",
        "        # Move to GPU if available\n",
        "        if res is not None:\n",
        "            print(\"Using GPU acceleration for FAISS\")\n",
        "            self.index_ip = faiss.index_cpu_to_gpu(res, 0, self.index_ip)\n",
        "            self.index_l2 = faiss.index_cpu_to_gpu(res, 0, self.index_l2)\n",
        "\n",
        "        # Add embeddings to indices\n",
        "        self.index_ip.add(self.embeddings.astype('float32'))\n",
        "        self.index_l2.add(self.embeddings.astype('float32'))\n",
        "\n",
        "        # Create HNSW indices for faster retrieval (approximate but faster)\n",
        "        print(\"Creating HNSW index for faster retrieval...\")\n",
        "        self.index_hnsw = faiss.IndexHNSWFlat(dimension, 32)  # 32 neighbors per node\n",
        "        self.index_hnsw.add(self.embeddings.astype('float32'))\n",
        "\n",
        "        # Sparse Index (CPU only)\n",
        "        print(\"Creating sparse BM25 index...\")\n",
        "        tokenized_corpus = [doc.split() for doc in texts]\n",
        "        self.bm25_index = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "        # TF-IDF index for additional sparse retrieval\n",
        "        print(\"Creating TF-IDF index...\")\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
        "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)\n",
        "\n",
        "    def search(self,\n",
        "               query: str,\n",
        "               k: int = 5,\n",
        "               method: str = \"hybrid\",\n",
        "               rerank: bool = False,\n",
        "               distance_metric: str = \"cosine\") -> Tuple[List[Document], float]:\n",
        "        \"\"\"\n",
        "        Search with timing and detailed logging\n",
        "\n",
        "        Args:\n",
        "            query: Query string\n",
        "            k: Number of results to return\n",
        "            method: Retrieval method (dense, sparse, hybrid, hnsw, tfidf)\n",
        "            rerank: Whether to apply cross-encoder reranking\n",
        "            distance_metric: For dense retrieval (cosine, l2)\n",
        "\n",
        "        Returns:\n",
        "            Retrieved documents and elapsed time\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        query = DataProcessor.preprocess_for_query(query)\n",
        "\n",
        "        if method == \"dense\":\n",
        "            results = self._dense_search(query, k, distance_metric)\n",
        "        elif method == \"sparse\":\n",
        "            results = self._sparse_search(query, k)\n",
        "        elif method == \"hnsw\":\n",
        "            results = self._hnsw_search(query, k)\n",
        "        elif method == \"tfidf\":\n",
        "            results = self._tfidf_search(query, k)\n",
        "        else:  # hybrid\n",
        "            results = self._hybrid_search(query, k)\n",
        "\n",
        "        # Apply reranking if requested\n",
        "        if rerank and len(results) > 1:\n",
        "            results = self._rerank_results(query, results, k)\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # Print retrieval results\n",
        "        print(f\"\\nRetrieved {len(results)} documents in {elapsed:.3f}s using {method} search\" +\n",
        "              f\" with{'' if rerank else 'out'} reranking:\")\n",
        "\n",
        "        for i, doc in enumerate(results[:3], 1):  # Show top 3 for brevity\n",
        "            page_number = doc.metadata.get('page', 'Unknown')\n",
        "            print(f\"[Doc {i}] Page {page_number}\\n{textwrap.shorten(doc.page_content, width=200)}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "        return results, elapsed\n",
        "\n",
        "    def _dense_search(self, query: str, k: int, distance_metric: str = \"cosine\") -> List[Document]:\n",
        "        \"\"\"Dense embedding-based search\"\"\"\n",
        "        embedding = self.dense_model.encode([query], device=str(device))[0]\n",
        "        embedding = embedding.astype('float32').reshape(1, -1)\n",
        "\n",
        "        if distance_metric == \"cosine\":\n",
        "            # Inner product search (for normalized vectors, equivalent to cosine similarity)\n",
        "            _, indices = self.index_ip.search(embedding, k)\n",
        "        else:  # l2\n",
        "            # Euclidean distance search\n",
        "            _, indices = self.index_l2.search(embedding, k)\n",
        "\n",
        "        return [self.chunks[i] for i in indices[0] if i != -1]\n",
        "\n",
        "    def _sparse_search(self, query: str, k: int) -> List[Document]:\n",
        "        \"\"\"BM25-based sparse search\"\"\"\n",
        "        tokenized = query.split()\n",
        "        scores = self.bm25_index.get_scores(tokenized)\n",
        "        indices = np.argsort(scores)[::-1][:k]\n",
        "        return [self.chunks[i] for i in indices]\n",
        "\n",
        "    def _hnsw_search(self, query: str, k: int) -> List[Document]:\n",
        "        \"\"\"HNSW approximate nearest neighbor search (faster but may be less accurate)\"\"\"\n",
        "        embedding = self.dense_model.encode([query], device=str(device))[0]\n",
        "        _, indices = self.index_hnsw.search(np.array([embedding]).astype('float32'), k)\n",
        "        return [self.chunks[i] for i in indices[0] if i != -1]\n",
        "\n",
        "    def _tfidf_search(self, query: str, k: int) -> List[Document]:\n",
        "        \"\"\"TF-IDF based search\"\"\"\n",
        "        query_vec = self.tfidf_vectorizer.transform([query])\n",
        "        # Calculate cosine similarity between query and documents\n",
        "        from sklearn.metrics.pairwise import cosine_similarity\n",
        "        scores = cosine_similarity(query_vec, self.tfidf_matrix)[0]\n",
        "        indices = np.argsort(scores)[::-1][:k]\n",
        "        return [self.chunks[i] for i in indices]\n",
        "\n",
        "    def _hybrid_search(self, query: str, k: int) -> List[Document]:\n",
        "        \"\"\"Hybrid search combining dense and sparse retrievals with RRF fusion\"\"\"\n",
        "        # Get more results than needed for fusion\n",
        "        dense_results = self._dense_search(query, k*2)\n",
        "        sparse_results = self._sparse_search(query, k*2)\n",
        "\n",
        "        # Reciprocal Rank Fusion\n",
        "        rrf_scores = {}\n",
        "\n",
        "        # Process dense results\n",
        "        for rank, doc in enumerate(dense_results):\n",
        "            idx = self.chunks.index(doc)\n",
        "            rrf_scores[idx] = rrf_scores.get(idx, 0) + 1/(60 + rank)\n",
        "\n",
        "        # Process sparse results\n",
        "        for rank, doc in enumerate(sparse_results):\n",
        "            idx = self.chunks.index(doc)\n",
        "            rrf_scores[idx] = rrf_scores.get(idx, 0) + 1/(60 + rank)\n",
        "\n",
        "        # Sort by RRF score and get top k\n",
        "        sorted_indices = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
        "        return [self.chunks[idx] for idx, _ in sorted_indices]\n",
        "\n",
        "    def _rerank_results(self, query: str, results: List[Document], k: int) -> List[Document]:\n",
        "        \"\"\"Rerank results using cross-encoder\"\"\"\n",
        "        if not results:\n",
        "            return results\n",
        "\n",
        "        # Create query-document pairs for reranking\n",
        "        doc_texts = [doc.page_content for doc in results]\n",
        "        pairs = [[query, doc] for doc in doc_texts]\n",
        "\n",
        "        # Get cross-encoder scores\n",
        "        scores = self.cross_encoder.predict(pairs)\n",
        "\n",
        "        # Sort by score\n",
        "        doc_score_pairs = list(zip(results, scores))\n",
        "        reranked_results = [doc for doc, _ in sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)]\n",
        "\n",
        "        return reranked_results[:k]  # Return top k after reranking\n",
        "\n",
        "#%%\n",
        "### 4. Document Context Enhancement\n",
        "class ContextEnhancer:\n",
        "    \"\"\"\n",
        "    Enhance retrieved contexts before sending to LLM:\n",
        "    - Summarization\n",
        "    - Context reordering\n",
        "    - Highlighting key information\n",
        "    - Removing redundancy\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Load summarization model\n",
        "        print(\"Loading summarization model...\")\n",
        "        self.summarizer = hf_pipeline(\n",
        "            \"summarization\",\n",
        "            model=\"facebook/bart-large-cnn\",\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "    def enhance_context(self,\n",
        "                       documents: List[Document],\n",
        "                       query: str,\n",
        "                       methods: List[str] = None) -> Tuple[str, Dict]:\n",
        "        \"\"\"\n",
        "        Apply enhancement methods to retrieved documents\n",
        "\n",
        "        Args:\n",
        "            documents: List of retrieved documents\n",
        "            query: Original query\n",
        "            methods: List of enhancement methods to apply\n",
        "\n",
        "        Returns:\n",
        "            Enhanced context string and metadata about applied enhancements\n",
        "        \"\"\"\n",
        "        if not methods:\n",
        "            methods = []\n",
        "\n",
        "        print(f\"\\nEnhancing context using methods: {', '.join(methods) if methods else 'None'}\")\n",
        "\n",
        "        # Start with original context\n",
        "        context_texts = [doc.page_content for doc in documents]\n",
        "        context = \"\\n\\n\".join(context_texts)\n",
        "\n",
        "        metadata = {\n",
        "            \"original_length\": len(context),\n",
        "            \"methods_applied\": methods\n",
        "        }\n",
        "\n",
        "        # Apply enhancements in sequence\n",
        "        if \"reorder\" in methods:\n",
        "            context, reorder_meta = self._reorder_by_relevance(context_texts, query)\n",
        "            metadata[\"reordering\"] = reorder_meta\n",
        "\n",
        "        if \"summarize\" in methods:\n",
        "            context, summary_meta = self._summarize_context(context, query)\n",
        "            metadata[\"summarization\"] = summary_meta\n",
        "\n",
        "        if \"highlight\" in methods:\n",
        "            context, highlight_meta = self._highlight_key_info(context, query)\n",
        "            metadata[\"highlighting\"] = highlight_meta\n",
        "\n",
        "        if \"deduplicate\" in methods:\n",
        "            context, dedup_meta = self._remove_redundancy(context)\n",
        "            metadata[\"deduplication\"] = dedup_meta\n",
        "\n",
        "        metadata[\"final_length\"] = len(context)\n",
        "\n",
        "        print(f\"Context enhancement complete: {metadata['original_length']} chars → \" +\n",
        "              f\"{metadata['final_length']} chars ({metadata['final_length']/metadata['original_length']:.1%})\")\n",
        "\n",
        "        return context, metadata\n",
        "\n",
        "    def _summarize_context(self, context: str, query: str) -> Tuple[str, Dict]:\n",
        "        \"\"\"Generate concise summaries to reduce context length\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # For long contexts, chunk and summarize separately\n",
        "        if len(context) > 1024:\n",
        "            chunks = textwrap.wrap(context, 1024, break_long_words=False, break_on_hyphens=False)\n",
        "            summaries = []\n",
        "\n",
        "            for chunk in chunks:\n",
        "                result = self.summarizer(\n",
        "                    chunk,\n",
        "                    max_length=min(len(chunk)//4, 256),\n",
        "                    min_length=min(len(chunk)//8, 100),\n",
        "                    do_sample=False\n",
        "                )\n",
        "                summaries.append(result[0]['summary_text'])\n",
        "\n",
        "            summary = \"\\n\\n\".join(summaries)\n",
        "        else:\n",
        "            result = self.summarizer(\n",
        "                context,\n",
        "                max_length=min(len(context)//4, 256),\n",
        "                min_length=min(len(context)//8, 100),\n",
        "                do_sample=False\n",
        "            )\n",
        "            summary = result[0]['summary_text']\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        metadata = {\n",
        "            \"original_length\": len(context),\n",
        "            \"summary_length\": len(summary),\n",
        "            \"reduction_pct\": 1 - (len(summary) / len(context)),\n",
        "            \"time_taken\": elapsed\n",
        "        }\n",
        "\n",
        "        # Add prefix to make it clear this is a summary\n",
        "        prefixed_summary = f\"SUMMARY OF RETRIEVED INFORMATION:\\n{summary}\\n\\nFULL CONTEXT:\\n{context}\"\n",
        "\n",
        "        return prefixed_summary, metadata\n",
        "\n",
        "    def _reorder_by_relevance(self, contexts: List[str], query: str) -> Tuple[str, Dict]:\n",
        "        \"\"\"Reorder context chunks by relevance to query\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Use NLTK to break into sentences for more fine-grained reordering\n",
        "        all_sentences = []\n",
        "        sentence_to_chunk = {}\n",
        "\n",
        "        for chunk_idx, chunk in enumerate(contexts):\n",
        "            sentences = sent_tokenize(chunk)\n",
        "            for sent in sentences:\n",
        "                if len(sent.strip()) > 0:\n",
        "                    all_sentences.append(sent)\n",
        "                    sentence_to_chunk[sent] = chunk_idx\n",
        "\n",
        "        # Create a relevance scorer using query-based heuristics\n",
        "        def relevance_score(sentence, query):\n",
        "            # Very simple relevance measure - count query terms in sentence\n",
        "            score = 0\n",
        "            query_terms = set(query.lower().split())\n",
        "            for term in query_terms:\n",
        "                if term in sentence.lower():\n",
        "                    score += 1\n",
        "            return score\n",
        "\n",
        "        # Score and sort sentences\n",
        "        sentence_scores = [(sent, relevance_score(sent, query)) for sent in all_sentences]\n",
        "        sorted_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Reconstruct chunks in order of relevance\n",
        "        chunk_relevance = {}\n",
        "        for sent, score in sentence_scores:\n",
        "            chunk_idx = sentence_to_chunk[sent]\n",
        "            chunk_relevance[chunk_idx] = chunk_relevance.get(chunk_idx, 0) + score\n",
        "\n",
        "        sorted_chunks = sorted([(idx, contexts[idx], rel) for idx, rel in chunk_relevance.items()],\n",
        "                                key=lambda x: x[2], reverse=True)\n",
        "\n",
        "        reordered_context = \"\\n\\n\".join([chunk for _, chunk, _ in sorted_chunks])\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        metadata = {\n",
        "            \"chunk_scores\": {idx: score for idx, _, score in sorted_chunks},\n",
        "            \"time_taken\": elapsed\n",
        "        }\n",
        "\n",
        "        return reordered_context, metadata\n",
        "\n",
        "    def _highlight_key_info(self, context: str, query: str) -> Tuple[str, Dict]:\n",
        "        \"\"\"Highlight key information related to the query\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Simple approach: highlight sentences containing query terms\n",
        "        query_terms = set(query.lower().split())\n",
        "        sentences = sent_tokenize(context)\n",
        "\n",
        "        highlighted_sentences = []\n",
        "        for sentence in sentences:\n",
        "            should_highlight = any(term in sentence.lower() for term in query_terms)\n",
        "            if should_highlight:\n",
        "                highlighted_sentences.append(f\"*IMPORTANT:* {sentence}\")\n",
        "            else:\n",
        "                highlighted_sentences.append(sentence)\n",
        "\n",
        "        highlighted_context = \" \".join(highlighted_sentences)\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        metadata = {\n",
        "            \"highlights_added\": sum(1 for s in highlighted_sentences if s.startswith(\"*IMPORTANT:*\")),\n",
        "            \"time_taken\": elapsed\n",
        "        }\n",
        "\n",
        "        return highlighted_context, metadata\n",
        "\n",
        "    def _remove_redundancy(self, context: str) -> Tuple[str, Dict]:\n",
        "        \"\"\"Remove redundant information from context\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Split into sentences\n",
        "        sentences = sent_tokenize(context)\n",
        "\n",
        "        # Simple deduplication: remove exact or near-duplicate sentences\n",
        "        unique_sentences = []\n",
        "        seen = set()\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # Create a simplified fingerprint of the sentence\n",
        "            fingerprint = re.sub(r'[^\\w]', '', sentence.lower())\n",
        "\n",
        "            # Keep only if we haven't seen a very similar fingerprint\n",
        "            if fingerprint not in seen:\n",
        "                unique_sentences.append(sentence)\n",
        "                seen.add(fingerprint)\n",
        "\n",
        "        deduplicated_context = \" \".join(unique_sentences)\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        metadata = {\n",
        "            \"original_sentences\": len(sentences),\n",
        "            \"deduplicated_sentences\": len(unique_sentences),\n",
        "            \"reduction_pct\": 1 - (len(unique_sentences) / len(sentences)),\n",
        "            \"time_taken\": elapsed\n",
        "        }\n",
        "\n",
        "        return deduplicated_context, metadata\n",
        "\n",
        "#%%\n",
        "### 5. Generation Models\n",
        "class AdvancedAnswerGenerator:\n",
        "    \"\"\"\n",
        "    Enhanced answer generation with multiple model options and prompt strategies\n",
        "    - Multiple model choices\n",
        "    - Different prompt techniques\n",
        "    - Chain-of-thought prompting\n",
        "    - Few-shot prompting\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._loaded_models = {}  # Cache for dynamically loaded models\n",
        "\n",
        "    def _get_model(self, model_type: str) -> Any:\n",
        "        \"\"\"Load model on demand and cache it\"\"\"\n",
        "        if model_type not in self._loaded_models:\n",
        "            print(f\"Loading {model_type} model...\")\n",
        "\n",
        "            if model_type == \"qwen\":\n",
        "                self._loaded_models[model_type] = hf_pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=\"Qwen/Qwen1.5-0.5B-Chat\",\n",
        "                    device_map=\"auto\",\n",
        "                    torch_dtype=\"auto\",\n",
        "                    model_kwargs={\"cache_dir\": CACHE_DIR}\n",
        "                )\n",
        "            elif model_type == \"flan-t5\":\n",
        "                self._loaded_models[model_type] = hf_pipeline(\n",
        "                    \"text2text-generation\",\n",
        "                    model=\"google/flan-t5-base\",\n",
        "                    device_map=\"auto\",\n",
        "                    model_kwargs={\"cache_dir\": CACHE_DIR}\n",
        "                )\n",
        "            elif model_type == \"tiny-llama\":\n",
        "                self._loaded_models[model_type] = hf_pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "                    device_map=\"auto\",\n",
        "                    torch_dtype=\"auto\",\n",
        "                    model_kwargs={\"cache_dir\": CACHE_DIR}\n",
        "                )\n",
        "            elif model_type == \"phi-2\":\n",
        "                self._loaded_models[model_type] = hf_pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=\"microsoft/phi-2\",\n",
        "                    device_map=\"auto\",\n",
        "                    torch_dtype=\"auto\",\n",
        "                    model_kwargs={\"cache_dir\": CACHE_DIR}\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "        return self._loaded_models[model_type]\n",
        "\n",
        "    def generate(self,\n",
        "                query: str,\n",
        "                context: str,\n",
        "                model_type: str = \"qwen\",\n",
        "                prompt_strategy: str = \"standard\",\n",
        "                max_new_tokens: int = 500,\n",
        "                temperature: float = 0.7) -> Tuple[str, float]:\n",
        "        \"\"\"\n",
        "        Generate answer with timing and different prompt strategies\n",
        "\n",
        "        Args:\n",
        "            query: User query\n",
        "            context: Retrieved context\n",
        "            model_type: Model to use for generation\n",
        "            prompt_strategy: Prompt technique (standard, cot, few_shot)\n",
        "            max_new_tokens: Maximum number of tokens to generate\n",
        "            temperature: Sampling temperature\n",
        "\n",
        "        Returns:\n",
        "            Generated answer and elapsed time\n",
        "        \"\"\"\n",
        "        model = self._get_model(model_type)\n",
        "        prompt = self._format_prompt(query, context, model_type, prompt_strategy)\n",
        "\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            if model_type == \"flan-t5\":\n",
        "                result = model(prompt, max_length=max_new_tokens)[0]['generated_text']\n",
        "            else:\n",
        "                result = model(\n",
        "                    prompt,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    do_sample=True,\n",
        "                    temperature=temperature\n",
        "                )[0]['generated_text']\n",
        "\n",
        "                # Post-process to extract the actual answer\n",
        "                if model_type in [\"qwen\", \"tiny-llama\"]:\n",
        "                    # Extract content between assistant tags\n",
        "                    match = re.search(r'<\\|im_start\\|>assistant\\n(.*?)(?:<\\|im_end\\|>|$)', result, re.DOTALL)\n",
        "                    if match:\n",
        "                        result = match.group(1).strip()\n",
        "                elif model_type == \"phi-2\":\n",
        "                    # Extract content after the prompt\n",
        "                    result = result.split(\"Answer:\")[-1].strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating answer: {str(e)}\")\n",
        "            result = f\"Error generating answer with {model_type}: {str(e)}\"\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"Generated answer in {elapsed:.3f}s using {model_type} with {prompt_strategy} prompting\")\n",
        "\n",
        "        # Log a preview of the answer\n",
        "        print(f\"Answer preview: {result[:100]}...\" if len(result) > 100 else f\"Answer: {result}\")\n",
        "\n",
        "        return result, elapsed\n",
        "\n",
        "    def _format_prompt(self, query: str, context: str, model_type: str, strategy: str) -> str:\n",
        "        \"\"\"\n",
        "        Format prompt according to model requirements and prompt strategy\n",
        "\n",
        "        Strategies:\n",
        "        - standard: Basic prompt with context and question\n",
        "        - cot: Chain-of-thought prompting\n",
        "        - few_shot: Few-shot examples\n",
        "        \"\"\"\n",
        "        # Limit context length based on model type\n",
        "        max_ctx_len = 4000  # Default\n",
        "        if model_type == \"flan-t5\":\n",
        "            max_ctx_len = 2000\n",
        "        elif model_type in [\"phi-2\", \"qwen\", \"tiny-llama\"]:\n",
        "            max_ctx_len = 3000\n",
        "\n",
        "        if len(context) > max_ctx_len:\n",
        "            context = context[:max_ctx_len] + \"...(context truncated due to length)\"\n",
        "\n",
        "        # Model-specific formatting\n",
        "        if model_type in [\"qwen\", \"tiny-llama\"]:\n",
        "            if strategy == \"cot\":\n",
        "                return f\"\"\"<|im_start|>system\n",
        "You are an AI assistant that answers questions based on the provided context. Think step by step before providing your final answer.\n",
        "Context: {context}<|im_end|>\n",
        "<|im_start|>user\n",
        "{query}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Let me think through this step by step:\n",
        "1. First, I'll understand what the question is asking.\n",
        "2. Then, I'll search the provided context for relevant information.\n",
        "3. Finally, I'll formulate a comprehensive answer based on the context.\n",
        "\n",
        "\"\"\"\n",
        "            elif strategy == \"few_shot\":\n",
        "                return f\"\"\"<|im_start|>system\n",
        "You are an AI assistant that answers questions based on the provided context.\n",
        "Context: {context}<|im_end|>\n",
        "<|im_start|>user\n",
        "What services are available to students?<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Based on the provided context, the services available to students include academic advising, counseling services, career services, library resources, and health services.<|im_end|>\n",
        "<|im_start|>user\n",
        "{query}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "            else:  # standard\n",
        "                return f\"\"\"<|im_start|>system\n",
        "You are an AI assistant that answers questions based on the provided context.\n",
        "Context: {context}<|im_end|>\n",
        "<|im_start|>user\n",
        "{query}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "        elif model_type == \"phi-2\":\n",
        "            if strategy == \"cot\":\n",
        "                return f\"\"\"Context: {context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Let me think through this step by step to find the answer in the context.\n",
        "\n",
        "Step 1: Understand what the question is asking.\n",
        "Step 2: Look through the context for relevant information.\n",
        "Step 3: Formulate my answer based on the context.\n",
        "\n",
        "Answer:\"\"\"\n",
        "            elif strategy == \"few_shot\":\n",
        "                return f\"\"\"Context: {context}\n",
        "\n",
        "Question: What services are available to students?\n",
        "Answer: Based on the provided context, the services available to students include academic advising, counseling services, career services, library resources, and health services.\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "            else:  # standard\n",
        "                return f\"\"\"Context: {context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "        else:  # flan-t5\n",
        "            if strategy == \"cot\":\n",
        "                return f\"context: {context}\\nquestion: {query}\\nThink step by step to answer the question:\"\n",
        "            elif strategy == \"few_shot\":\n",
        "                return f\"context: {context}\\nquestion: What services are available to students?\\nanswer: Based on the provided context, the services available to students include academic advising, counseling services, career services, library resources, and health services.\\nquestion: {query}\\nanswer:\"\n",
        "            else:  # standard\n",
        "                return f\"context: {context}\\nquestion: {query}\\nanswer:\"\n",
        "\n",
        "#%%\n",
        "### 6. Enhanced Evaluation System\n",
        "class AdvancedRagEvaluator:\n",
        "    \"\"\"\n",
        "    Enhanced evaluation system with multiple metrics:\n",
        "    - Relevance\n",
        "    - Faithfulness\n",
        "    - Conciseness\n",
        "    - Answer quality\n",
        "    - Consistency\n",
        "    - Latency\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"Initializing advanced evaluator...\")\n",
        "        self._evaluator = None\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    @property\n",
        "    def evaluator(self):\n",
        "        \"\"\"Lazy-load evaluation model\"\"\"\n",
        "        if self._evaluator is None:\n",
        "            print(\"Loading evaluation model...\")\n",
        "            self._evaluator = hf_pipeline(\n",
        "                \"text-generation\",\n",
        "                model=\"Qwen/Qwen1.5-0.5B-Chat\",\n",
        "                device_map=\"auto\",\n",
        "                torch_dtype=\"auto\",\n",
        "                model_kwargs={\"cache_dir\": CACHE_DIR}\n",
        "            )\n",
        "        return self._evaluator\n",
        "\n",
        "    def evaluate(self, answer: str, context: str, query: str, full_metrics: bool = True) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Enhanced evaluation with multiple scoring metrics\n",
        "\n",
        "        Args:\n",
        "            answer: Generated answer\n",
        "            context: Context used for generation\n",
        "            query: Original query\n",
        "            full_metrics: Whether to calculate all metrics (slower) or basic ones\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of evaluation metrics\n",
        "        \"\"\"\n",
        "        print(\"\\nEvaluating answer quality...\")\n",
        "        metrics = {}\n",
        "\n",
        "        # Always calculate basic metrics\n",
        "        relevance_score = self._get_llm_score(\n",
        "            f\"Rate answer relevance to question (1-5, 5=most relevant):\\nQuestion: {query}\\nAnswer: {answer}\\nScore:\"\n",
        "        )\n",
        "        faithfulness_score = self._get_llm_score(\n",
        "            f\"Rate answer faithfulness to context (1-5, 5=most faithful):\\nContext: {context[:2000]}...\\nAnswer: {answer}\\nScore:\"\n",
        "        )\n",
        "\n",
        "        metrics[\"relevance\"] = relevance_score\n",
        "        metrics[\"faithfulness\"] = faithfulness_score\n",
        "        metrics[\"composite_score\"] = (relevance_score + faithfulness_score) / 2\n",
        "\n",
        "        # Calculate additional metrics if requested\n",
        "        if full_metrics:\n",
        "            # Conciseness\n",
        "            conciseness_score = self._get_llm_score(\n",
        "                f\"Rate answer conciseness (1-5, 5=most concise):\\nAnswer: {answer}\\nScore:\"\n",
        "            )\n",
        "            metrics[\"conciseness\"] = conciseness_score\n",
        "\n",
        "            # Completeness\n",
        "            completeness_score = self._get_llm_score(\n",
        "                f\"Rate how completely the answer addresses the question (1-5, 5=most complete):\\nQuestion: {query}\\nAnswer: {answer}\\nScore:\"\n",
        "            )\n",
        "            metrics[\"completeness\"] = completeness_score\n",
        "\n",
        "            # ROUGE scores (content overlap with context)\n",
        "            if context:\n",
        "                rouge_scores = self.rouge_scorer.score(answer, context[:2000])\n",
        "                metrics[\"rouge1\"] = rouge_scores['rouge1'].fmeasure\n",
        "                metrics[\"rouge2\"] = rouge_scores['rouge2'].fmeasure\n",
        "                metrics[\"rougeL\"] = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "            # Calculate overall quality\n",
        "            metrics[\"overall_quality\"] = (\n",
        "                metrics[\"relevance\"] +\n",
        "                metrics[\"faithfulness\"] +\n",
        "                metrics[\"conciseness\"] +\n",
        "                metrics[\"completeness\"]\n",
        "            ) / 4\n",
        "\n",
        "        # Log metrics\n",
        "        print(f\"Evaluation Scores:\")\n",
        "        for metric, value in metrics.items():\n",
        "            if isinstance(value, float):\n",
        "                print(f\"  - {metric}: {value:.2f}\")\n",
        "            else:\n",
        "                print(f\"  - {metric}: {value}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "\n",
        "\n",
        "    def _get_llm_score(self, prompt: str) -> float:\n",
        "        \"\"\"Helper method to extract score from LLM evaluation\"\"\"\n",
        "        try:\n",
        "            response = self.evaluator(\n",
        "                f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
        "                max_new_tokens=2,\n",
        "                do_sample=False\n",
        "            )[0]['generated_text'].strip()\n",
        "\n",
        "            # Extract first digit from response\n",
        "            for char in response:\n",
        "                if char.isdigit():\n",
        "                    return float(char)\n",
        "\n",
        "            # If no digit found, use regex to find numbers like \"4.5\" or \"3\"\n",
        "            matches = re.findall(r'\\d+(\\.\\d+)?', response)\n",
        "            if matches:\n",
        "                score = float(matches[0])\n",
        "                return min(5.0, max(1.0, score))  # Clamp between 1 and 5\n",
        "\n",
        "            return 3.0  # Default score if no digit found\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in evaluation: {str(e)}\")\n",
        "            return 3.0  # Default score on error\n",
        "\n",
        "#%%\n",
        "### 7. Complete RAG Pipeline with Comprehensive Grid Search\n",
        "class AdvancedRagPipeline:\n",
        "    \"\"\"\n",
        "    Memory-optimized RAG Pipeline with comprehensive grid search for optimal parameters:\n",
        "    - Multiple chunking strategies\n",
        "    - Multiple retrieval methods\n",
        "    - Multiple prompt strategies\n",
        "    - Multiple context enhancement methods\n",
        "    - Multiple LLM models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, file_paths: List[str]):\n",
        "        \"\"\"Initialize the RAG pipeline with multiple PDF files\"\"\"\n",
        "        self.file_paths = file_paths  # Store list of PDF paths\n",
        "        self.retriever = None\n",
        "        self._loaded_models = {}  # Cache for dynamically loaded models\n",
        "        self._context_enhancer = None\n",
        "        self._evaluator = None\n",
        "\n",
        "    @property\n",
        "    def context_enhancer(self):\n",
        "        \"\"\"Lazy-load context enhancer\"\"\"\n",
        "        if self._context_enhancer is None:\n",
        "            self._context_enhancer = ContextEnhancer()\n",
        "            torch.cuda.empty_cache()\n",
        "        return self._context_enhancer\n",
        "\n",
        "    @property\n",
        "    def evaluator(self):\n",
        "        \"\"\"Lazy-load evaluator\"\"\"\n",
        "        if self._evaluator is None:\n",
        "            self._evaluator = AdvancedRagEvaluator()\n",
        "            torch.cuda.empty_cache()\n",
        "        return self._evaluator\n",
        "\n",
        "    def prepare_for_demo(self):\n",
        "        \"\"\"Prepare the pipeline for interactive demo\"\"\"\n",
        "        if self.retriever is None:\n",
        "            chunks = DataProcessor.load_and_chunk(\n",
        "                self.file_paths,  # Use list of file paths\n",
        "                chunking_strategy=\"recursive\",\n",
        "                chunk_size=512,\n",
        "                chunk_overlap=64\n",
        "            )\n",
        "            self.retriever = AdvancedRetriever(chunks)\n",
        "\n",
        "    def _get_generator_model(self, model_type):\n",
        "        \"\"\"Load model on demand\"\"\"\n",
        "        key = f\"generator_{model_type}\"\n",
        "        if key not in self._loaded_models:\n",
        "            print(f\"Loading {model_type} model...\")\n",
        "\n",
        "            if model_type == \"qwen\":\n",
        "                self._loaded_models[key] = hf_pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=\"Qwen/Qwen1.5-0.5B-Chat\",\n",
        "                    device_map=\"auto\",\n",
        "                    torch_dtype=\"auto\",\n",
        "                    model_kwargs={\"cache_dir\": CACHE_DIR}\n",
        "                )\n",
        "            elif model_type == \"flan-t5\":\n",
        "                self._loaded_models[key] = hf_pipeline(\n",
        "                    \"text2text-generation\",\n",
        "                    model=\"google/flan-t5-base\",\n",
        "                    device_map=\"auto\",\n",
        "                    model_kwargs={\"cache_dir\": CACHE_DIR}\n",
        "                )\n",
        "            elif model_type == \"tiny-llama\":\n",
        "                self._loaded_models[key] = hf_pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "                    device_map=\"auto\",\n",
        "                    torch_dtype=\"auto\",\n",
        "                    model_kwargs={\"cache_dir\": CACHE_DIR}\n",
        "                )\n",
        "            elif model_type == \"phi-2\":\n",
        "                self._loaded_models[key] = hf_pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=\"microsoft/phi-2\",\n",
        "                    device_map=\"auto\",\n",
        "                    torch_dtype=\"auto\",\n",
        "                    model_kwargs={\"cache_dir\": CACHE_DIR}\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return self._loaded_models[key]\n",
        "\n",
        "    def experiment(self, config: Dict) -> pd.DataFrame:\n",
        "        \"\"\"Run grid search experiment in memory-efficient batches\"\"\"\n",
        "        test_queries = config.get(\"test_queries\", [])\n",
        "\n",
        "        # Calculate total configurations\n",
        "        total_configs = self._calculate_total_configs(config)\n",
        "        print(f\"\\nStarting grid search with {total_configs} configurations...\")\n",
        "\n",
        "        # Store results\n",
        "        results = []\n",
        "        experiment_start = time.time()\n",
        "\n",
        "        # Process in batches by chunking strategy\n",
        "        for chunking_strategy in config[\"chunking_strategies\"]:\n",
        "            for chunk_size in config[\"chunk_sizes\"]:\n",
        "                for chunk_overlap in config[\"chunk_overlaps\"]:\n",
        "                    # Process documents for this chunk configuration\n",
        "                    chunks = DataProcessor.load_and_chunk(\n",
        "                        self.file_paths,  # Use list of file paths\n",
        "                        chunking_strategy=chunking_strategy,\n",
        "                        chunk_size=chunk_size,\n",
        "                        chunk_overlap=chunk_overlap\n",
        "                    )\n",
        "                    self.retriever = AdvancedRetriever(chunks)\n",
        "\n",
        "                    # Run configuration batches with this retriever\n",
        "                    batch_results = self._run_chunking_batch(\n",
        "                        config=config,\n",
        "                        chunk_size=chunk_size,\n",
        "                        chunk_overlap=chunk_overlap,\n",
        "                        chunking_strategy=chunking_strategy\n",
        "                    )\n",
        "                    results.extend(batch_results)\n",
        "\n",
        "                    # Clear memory\n",
        "                    self.retriever = None\n",
        "                    torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "                    # Save batch results to disk to free memory\n",
        "                    batch_df = pd.DataFrame(batch_results)\n",
        "                    batch_counter = len(results) // len(batch_results)\n",
        "                    batch_df.to_csv(f\"rag_results_batch_{batch_counter}.csv\", index=False)\n",
        "\n",
        "        total_experiment_time = time.time() - experiment_start\n",
        "        print(f\"\\nExperiment completed in {total_experiment_time:.2f} seconds\")\n",
        "\n",
        "        # Combine all batch results\n",
        "        results_df = self._combine_batch_results()\n",
        "        self._combine_batch_results()\n",
        "\n",
        "        return results_df\n",
        "    def _run_chunking_batch(self, config, chunk_size, chunk_overlap, chunking_strategy):\n",
        "        \"\"\"Run all configurations for a specific chunking strategy\"\"\"\n",
        "        batch_results = []\n",
        "\n",
        "        # Calculate total configs for this batch\n",
        "        total_batch_configs = (\n",
        "            len(config[\"retrieval_methods\"]) *\n",
        "            len(config[\"reranking_options\"]) *\n",
        "            len(config[\"context_enhancers\"]) *\n",
        "            len(config[\"prompt_strategies\"]) *\n",
        "            len(config[\"models\"]) *\n",
        "            len(config[\"top_k_values\"]) *\n",
        "            len(config[\"test_queries\"])\n",
        "        )\n",
        "\n",
        "        with tqdm(total=total_batch_configs, desc=f\"Running {chunking_strategy} chunks\") as pbar:\n",
        "            # Loop through parameter combinations for this chunking strategy\n",
        "            for method in config[\"retrieval_methods\"]:\n",
        "                for rerank in config[\"reranking_options\"]:\n",
        "                    for context_enhancers in config[\"context_enhancers\"]:\n",
        "                        for prompt_strategy in config[\"prompt_strategies\"]:\n",
        "                            # Group by model to avoid loading/unloading too frequently\n",
        "                            for model_type in config[\"models\"]:\n",
        "                                # Load model once for this batch\n",
        "                                model = self._get_generator_model(model_type)\n",
        "\n",
        "                                for top_k in config[\"top_k_values\"]:\n",
        "                                    for query in config[\"test_queries\"]:\n",
        "                                        result = self._run_configuration(\n",
        "                                            query=query,\n",
        "                                            chunk_size=chunk_size,\n",
        "                                            chunk_overlap=chunk_overlap,\n",
        "                                            chunking_strategy=chunking_strategy,\n",
        "                                            method=method,\n",
        "                                            rerank=rerank,\n",
        "                                            context_enhancers=context_enhancers,\n",
        "                                            prompt_strategy=prompt_strategy,\n",
        "                                            model_type=model_type,\n",
        "                                            top_k=top_k\n",
        "                                        )\n",
        "                                        batch_results.append(result)\n",
        "                                        pbar.update(1)\n",
        "\n",
        "                                # Explicitly clean up after all configs for this model\n",
        "                                torch.cuda.empty_cache()\n",
        "                                gc.collect()\n",
        "\n",
        "        return batch_results\n",
        "\n",
        "    def _calculate_total_configs(self, config):\n",
        "        \"\"\"Calculate total number of configurations\"\"\"\n",
        "        return (\n",
        "            len(config[\"chunk_sizes\"]) *\n",
        "            len(config[\"chunk_overlaps\"]) *\n",
        "            len(config[\"chunking_strategies\"]) *\n",
        "            len(config[\"retrieval_methods\"]) *\n",
        "            len(config[\"reranking_options\"]) *\n",
        "            len(config[\"context_enhancers\"]) *\n",
        "            len(config[\"prompt_strategies\"]) *\n",
        "            len(config[\"models\"]) *\n",
        "            len(config[\"top_k_values\"]) *\n",
        "            len(config[\"test_queries\"])\n",
        "        )\n",
        "\n",
        "    def _run_configuration(self,\n",
        "                          query: str,\n",
        "                          chunk_size: int,\n",
        "                          chunk_overlap: int,\n",
        "                          chunking_strategy: str,\n",
        "                          method: str,\n",
        "                          rerank: bool,\n",
        "                          context_enhancers: List[str],\n",
        "                          prompt_strategy: str,\n",
        "                          model_type: str,\n",
        "                          top_k: int) -> Dict:\n",
        "        \"\"\"\n",
        "        Execute single configuration run with full metrics\n",
        "\n",
        "        Args:\n",
        "            query: User query\n",
        "            chunk_size: Size of chunks\n",
        "            chunk_overlap: Overlap between chunks\n",
        "            chunking_strategy: Strategy for chunking\n",
        "            method: Retrieval method\n",
        "            rerank: Whether to apply reranking\n",
        "            context_enhancers: List of context enhancement methods\n",
        "            prompt_strategy: Prompt strategy\n",
        "            model_type: Model for generation\n",
        "            top_k: Number of documents to retrieve\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with results\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'-'*80}\")\n",
        "        print(f\"Running configuration:\")\n",
        "        print(f\"- Query: {query}\")\n",
        "        print(f\"- Chunking: {chunking_strategy} (size={chunk_size}, overlap={chunk_overlap})\")\n",
        "        print(f\"- Retrieval: {method} (rerank={rerank}, top_k={top_k})\")\n",
        "        print(f\"- Enhancement: {context_enhancers}\")\n",
        "        print(f\"- Generation: {model_type} with {prompt_strategy} prompting\")\n",
        "        print(f\"{'-'*80}\")\n",
        "\n",
        "        # Track timing\n",
        "        phase_times = {}\n",
        "\n",
        "        # 1. Retrieval phase\n",
        "        start_time = time.time()\n",
        "        contexts, retrieval_time = self.retriever.search(\n",
        "            query=query,\n",
        "            k=top_k,\n",
        "            method=method,\n",
        "            rerank=rerank\n",
        "        )\n",
        "        phase_times[\"retrieval\"] = retrieval_time\n",
        "\n",
        "        # 2. Context enhancement phase\n",
        "        start_time = time.time()\n",
        "        if context_enhancers:\n",
        "            context_str, enhancement_meta = self.context_enhancer.enhance_context(\n",
        "                documents=contexts,\n",
        "                query=query,\n",
        "                methods=context_enhancers\n",
        "            )\n",
        "        else:\n",
        "            context_str = \"\\n\\n\".join([c.page_content for c in contexts])\n",
        "            enhancement_meta = {\"original_length\": len(context_str), \"final_length\": len(context_str)}\n",
        "\n",
        "        phase_times[\"enhancement\"] = time.time() - start_time\n",
        "\n",
        "        # 3. Generation phase\n",
        "        answer, generation_time = self._generate_answer(\n",
        "            query=query,\n",
        "            context=context_str,\n",
        "            model_type=model_type,\n",
        "            prompt_strategy=prompt_strategy\n",
        "        )\n",
        "        phase_times[\"generation\"] = generation_time\n",
        "\n",
        "        # 4. Evaluation phase\n",
        "        start_time = time.time()\n",
        "        metrics = self.evaluator.evaluate(\n",
        "            answer=answer,\n",
        "            context=context_str,\n",
        "            query=query\n",
        "        )\n",
        "        phase_times[\"evaluation\"] = time.time() - start_time\n",
        "\n",
        "        # 5. Compile results\n",
        "        result = {\n",
        "            \"query\": query,\n",
        "            \"chunk_size\": chunk_size,\n",
        "            \"chunk_overlap\": chunk_overlap,\n",
        "            \"chunking_strategy\": chunking_strategy,\n",
        "            \"retrieval_method\": method,\n",
        "            \"reranking\": \"Yes\" if rerank else \"No\",\n",
        "            \"context_enhancement\": \"+\".join(context_enhancers) if context_enhancers else \"None\",\n",
        "            \"prompt_strategy\": prompt_strategy,\n",
        "            \"model\": model_type,\n",
        "            \"top_k\": top_k,\n",
        "            \"context_length\": enhancement_meta[\"final_length\"],\n",
        "            \"answer_length\": len(answer),\n",
        "            \"relevance\": metrics[\"relevance\"],\n",
        "            \"faithfulness\": metrics[\"faithfulness\"],\n",
        "            \"composite_score\": metrics[\"composite_score\"],\n",
        "            \"retrieval_time\": phase_times[\"retrieval\"],\n",
        "            \"enhancement_time\": phase_times[\"enhancement\"],\n",
        "            \"generation_time\": phase_times[\"generation\"],\n",
        "            \"evaluation_time\": phase_times[\"evaluation\"],\n",
        "            \"total_time\": sum(phase_times.values()),\n",
        "            \"answer\": answer[:1000] + \"...\" if len(answer) > 1000 else answer  # Store truncated answer\n",
        "        }\n",
        "\n",
        "        # Add additional metrics if available\n",
        "        for key, value in metrics.items():\n",
        "            if key not in result:\n",
        "                result[key] = value\n",
        "\n",
        "        # Calculate efficiency scores\n",
        "        result[\"efficiency_score\"] = result[\"composite_score\"] / result[\"total_time\"]\n",
        "\n",
        "        print(f\"Configuration completed in {result['total_time']:.3f}s with composite score: {result['composite_score']:.2f}\")\n",
        "\n",
        "        # Clean up after each configuration\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _generate_answer(self, query: str, context: str, model_type: str, prompt_strategy: str) -> Tuple[str, float]:\n",
        "        \"\"\"Generate answer with memory management\"\"\"\n",
        "        model = self._get_generator_model(model_type)\n",
        "        prompt = self._format_prompt(query, context, model_type, prompt_strategy)\n",
        "\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            if model_type == \"flan-t5\":\n",
        "                result = model(prompt, max_length=500)[0]['generated_text']\n",
        "            else:\n",
        "                result = model(\n",
        "                    prompt,\n",
        "                    max_new_tokens=500,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.7\n",
        "                )[0]['generated_text']\n",
        "\n",
        "                # Post-process to extract the actual answer\n",
        "                if model_type in [\"qwen\", \"tiny-llama\"]:\n",
        "                    # Extract content between assistant tags\n",
        "                    match = re.search(r'<\\|im_start\\|>assistant\\n(.*?)(?:<\\|im_end\\|>|$)', result, re.DOTALL)\n",
        "                    if match:\n",
        "                        result = match.group(1).strip()\n",
        "                elif model_type == \"phi-2\":\n",
        "                    # Extract content after the prompt\n",
        "                    result = result.split(\"Answer:\")[-1].strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating answer: {str(e)}\")\n",
        "            result = f\"Error generating answer with {model_type}: {str(e)}\"\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        return result, elapsed\n",
        "\n",
        "    def _format_prompt(self, query: str, context: str, model_type: str, strategy: str) -> str:\n",
        "        \"\"\"Format prompt according to model requirements\"\"\"\n",
        "        # Limit context length based on model type\n",
        "        max_ctx_len = 4000  # Default\n",
        "        if model_type == \"flan-t5\":\n",
        "            max_ctx_len = 2000\n",
        "        elif model_type in [\"phi-2\", \"qwen\", \"tiny-llama\"]:\n",
        "            max_ctx_len = 3000\n",
        "\n",
        "        if len(context) > max_ctx_len:\n",
        "            context = context[:max_ctx_len] + \"...(context truncated due to length)\"\n",
        "\n",
        "        # Model-specific formatting\n",
        "        if model_type in [\"qwen\", \"tiny-llama\"]:\n",
        "            if strategy == \"cot\":\n",
        "                return f\"\"\"<|im_start|>system\n",
        "You are an AI assistant that answers questions based on the provided context. Think step by step before providing your final answer.\n",
        "Context: {context}<|im_end|>\n",
        "<|im_start|>user\n",
        "{query}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Let me think through this step by step:\n",
        "1. First, I'll understand what the question is asking.\n",
        "2. Then, I'll search the provided context for relevant information.\n",
        "3. Finally, I'll formulate a comprehensive answer based on the context.\n",
        "\n",
        "\"\"\"\n",
        "            elif strategy == \"few_shot\":\n",
        "                return f\"\"\"<|im_start|>system\n",
        "You are an AI assistant that answers questions based on the provided context.\n",
        "Context: {context}<|im_end|>\n",
        "<|im_start|>user\n",
        "What services are available to students?<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Based on the provided context, the services available to students include academic advising, counseling services, career services, library resources, and health services.<|im_end|>\n",
        "<|im_start|>user\n",
        "{query}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "            else:  # standard\n",
        "                return f\"\"\"<|im_start|>system\n",
        "You are an AI assistant that answers questions based on the provided context.\n",
        "Context: {context}<|im_end|>\n",
        "<|im_start|>user\n",
        "{query}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "        elif model_type == \"phi-2\":\n",
        "            if strategy == \"cot\":\n",
        "                return f\"\"\"Context: {context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Let me think through this step by step to find the answer in the context.\n",
        "\n",
        "Step 1: Understand what the question is asking.\n",
        "Step 2: Look through the context for relevant information.\n",
        "Step 3: Formulate my answer based on the context.\n",
        "\n",
        "Answer:\"\"\"\n",
        "            elif strategy == \"few_shot\":\n",
        "                return f\"\"\"Context: {context}\n",
        "\n",
        "Question: What services are available to students?\n",
        "Answer: Based on the provided context, the services available to students include academic advising, counseling services, career services, library resources, and health services.\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "            else:  # standard\n",
        "                return f\"\"\"Context: {context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "        else:  # flan-t5\n",
        "            if strategy == \"cot\":\n",
        "                return f\"context: {context}\\nquestion: {query}\\nThink step by step to answer the question:\"\n",
        "            elif strategy == \"few_shot\":\n",
        "                return f\"context: {context}\\nquestion: What services are available to students?\\nanswer: Based on the provided context, the services available to students include academic advising, counseling services, career services, library resources, and health services.\\nquestion: {query}\\nanswer:\"\n",
        "            else:  # standard\n",
        "                return f\"context: {context}\\nquestion: {query}\\nanswer:\"\n",
        "\n",
        "    def _combine_batch_results(self) -> pd.DataFrame:\n",
        "        \"\"\"Combine all batch results from disk\"\"\"\n",
        "        batch_files = [f for f in os.listdir() if f.startswith(\"rag_results_batch_\")]\n",
        "        if not batch_files:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        dfs = []\n",
        "        for batch_file in batch_files:\n",
        "            try:\n",
        "                df = pd.read_csv(batch_file)\n",
        "                dfs.append(df)\n",
        "                os.remove(batch_file)  # Clean up after loading\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading batch file {batch_file}: {str(e)}\")\n",
        "\n",
        "        if dfs:\n",
        "            return pd.concat(dfs, ignore_index=True)\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    def _analyze_results(self, results_df: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Analyze grid search results and generate visualizations\n",
        "\n",
        "        Args:\n",
        "            results_df: DataFrame with experiment results\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EXPERIMENT RESULTS ANALYSIS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Calculate summary statistics\n",
        "        print(\"\\n--- Best Configurations by Composite Score ---\")\n",
        "        top_configs = results_df.sort_values('composite_score', ascending=False).head(5)\n",
        "        for i, (_, row) in enumerate(top_configs.iterrows(), 1):\n",
        "            print(f\"\\n{i}. Composite Score: {row['composite_score']:.2f}\")\n",
        "            print(f\"   Query: {row['query']}\")\n",
        "            print(f\"   Chunking: {row['chunking_strategy']} (size={row['chunk_size']}, overlap={row['chunk_overlap']})\")\n",
        "            print(f\"   Retrieval: {row['retrieval_method']} (rerank={row['reranking']}, top_k={row['top_k']})\")\n",
        "            print(f\"   Enhancement: {row['context_enhancement']}\")\n",
        "            print(f\"   Generation: {row['model']} with {row['prompt_strategy']} prompting\")\n",
        "            print(f\"   Times: Retrieval={row['retrieval_time']:.3f}s, Generation={row['generation_time']:.3f}s, Total={row['total_time']:.3f}s\")\n",
        "\n",
        "        print(\"\\n--- Best Configurations by Efficiency Score ---\")\n",
        "        top_efficient = results_df.sort_values('efficiency_score', ascending=False).head(5)\n",
        "        for i, (_, row) in enumerate(top_efficient.iterrows(), 1):\n",
        "            print(f\"\\n{i}. Efficiency Score: {row['efficiency_score']:.2f}\")\n",
        "            print(f\"   Query: {row['query']}\")\n",
        "            print(f\"   Chunking: {row['chunking_strategy']} (size={row['chunk_size']}, overlap={row['chunk_overlap']})\")\n",
        "            print(f\"   Retrieval: {row['retrieval_method']} (rerank={row['reranking']}, top_k={row['top_k']})\")\n",
        "            print(f\"   Enhancement: {row['context_enhancement']}\")\n",
        "            print(f\"   Generation: {row['model']} with {row['prompt_strategy']} prompting\")\n",
        "            print(f\"   Times: Retrieval={row['retrieval_time']:.3f}s, Generation={row['generation_time']:.3f}s, Total={row['total_time']:.3f}s\")\n",
        "\n",
        "        # Analyze performance by component\n",
        "        print(\"\\n--- Performance by Model ---\")\n",
        "        model_perf = results_df.groupby('model')[['relevance', 'faithfulness', 'composite_score', 'total_time']].mean()\n",
        "        print(model_perf)\n",
        "\n",
        "        print(\"\\n--- Performance by Retrieval Method ---\")\n",
        "        retrieval_perf = results_df.groupby('retrieval_method')[['relevance', 'faithfulness', 'composite_score', 'total_time']].mean()\n",
        "        print(retrieval_perf)\n",
        "\n",
        "        print(\"\\n--- Performance by Chunking Strategy ---\")\n",
        "        chunking_perf = results_df.groupby('chunking_strategy')[['relevance', 'faithfulness', 'composite_score', 'total_time']].mean()\n",
        "        print(chunking_perf)\n",
        "\n",
        "        print(\"\\n--- Performance by Context Enhancement ---\")\n",
        "        enhancement_perf = results_df.groupby('context_enhancement')[['relevance', 'faithfulness', 'composite_score', 'total_time']].mean()\n",
        "        print(enhancement_perf)\n",
        "\n",
        "        print(\"\\n--- Performance by Prompt Strategy ---\")\n",
        "        prompt_perf = results_df.groupby('prompt_strategy')[['relevance', 'faithfulness', 'composite_score', 'total_time']].mean()\n",
        "        print(prompt_perf)\n",
        "\n",
        "        # Save results\n",
        "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        results_path = f\"rag_experiment_results_{timestamp}.csv\"\n",
        "        results_df.to_csv(results_path, index=False)\n",
        "        print(f\"\\nResults saved to {results_path}\")\n",
        "\n",
        "        # Generate visualizations if matplotlib is available\n",
        "        try:\n",
        "            # Correlation heatmap\n",
        "            numeric_df = results_df.select_dtypes(include=[np.number])\n",
        "            plt.figure(figsize=(12, 10))\n",
        "            sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "            plt.title('Correlation Between Metrics')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"rag_correlation_heatmap_{timestamp}.png\")\n",
        "\n",
        "            # Performance by model and retrieval method\n",
        "            plt.figure(figsize=(14, 8))\n",
        "            performance_pivot = results_df.pivot_table(\n",
        "                index='model',\n",
        "                columns='retrieval_method',\n",
        "                values='composite_score',\n",
        "                aggfunc='mean'\n",
        "            )\n",
        "            sns.heatmap(performance_pivot, annot=True, cmap='viridis', fmt=\".2f\", linewidths=0.5)\n",
        "            plt.title('Average Composite Score by Model and Retrieval Method')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"rag_model_retrieval_performance_{timestamp}.png\")\n",
        "\n",
        "            print(f\"Visualizations saved as PNG files\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not generate visualizations: {str(e)}\")\n",
        "\n",
        "#%%\n",
        "### 8. Interactive demo mode\n",
        "def run_interactive_demo(pipeline, retrieval_method=\"hybrid\", model_type=\"qwen\", top_k=5):\n",
        "    \"\"\"\n",
        "    Run an interactive demo of the RAG system\n",
        "\n",
        "    Args:\n",
        "        pipeline: Configured RAG pipeline\n",
        "        retrieval_method: Method for retrieval\n",
        "        model_type: Model for generation\n",
        "        top_k: Number of documents to retrieve\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Interactive RAG System Demo\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Type 'exit' to quit\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\nEnter your question: \")\n",
        "        if query.lower() in ['exit', 'quit']:\n",
        "            break\n",
        "\n",
        "        # Retrieval phase\n",
        "        contexts, retrieval_time = pipeline.retriever.search(query, k=top_k, method=retrieval_method)\n",
        "        context_str = \"\\n\\n\".join([c.page_content for c in contexts])\n",
        "\n",
        "        # Generation phase\n",
        "        answer, generation_time = pipeline._generate_answer(query, context_str, model_type, \"standard\")\n",
        "\n",
        "        # Print results\n",
        "        print(f\"\\nAnswer (generated in {generation_time:.2f}s):\")\n",
        "        print(\"-\" * 80)\n",
        "        print(answer)\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"Total response time: {retrieval_time + generation_time:.2f}s\")\n",
        "\n",
        "#%%\n",
        "### 9. Execution & Analysis\n",
        "if __name__ == \"__main__\":\n",
        "    # PDF file path - replace with your own PDF path\n",
        "    PDF_FILES = [\n",
        "            \"/content/13. Atlas of Diabetes Mellitus (3rd Edition).pdf\",\n",
        "            \"/content/diabetes.pdf\"  # Replace with your second PDF path\n",
        "        ]\n",
        "    # Full experimental configuration with comprehensive parameter search\n",
        "    COMPREHENSIVE_CONFIG = {\n",
        "        # Test queries to evaluate system performance\n",
        "        \"test_queries\": [\n",
        "           # \"What is the policy for academic probation?\",\n",
        "            #\"What are the requirements for graduation?\",\n",
        "            #\"How can students access mental health services?\",\n",
        "            #\"What are the rules regarding academic dishonesty?\",\n",
        "            #\"How can I apply for a leave of absence?\"\n",
        "           \" what are the parking rules and regulation such that I dont get a fine ?\"\n",
        "        ],\n",
        "\n",
        "        # Chunking parameters\n",
        "        \"chunk_sizes\": [256, 512, 1024],\n",
        "        \"chunk_overlaps\": [32, 64, 128],\n",
        "        \"chunking_strategies\": [\"recursive\", \"sentence\", \"paragraph\"],\n",
        "\n",
        "        # Retrieval parameters\n",
        "        \"retrieval_methods\": [\"dense\", \"sparse\", \"hybrid\",\"tfidf\"],\n",
        "        #\"retrieval_methods\": [\"dense\", \"sparse\", \"hybrid\", \"hnsw\", \"tfidf\"],\n",
        "        \"reranking_options\": [True, False],\n",
        "        \"top_k_values\": [3, 5, 10],\n",
        "\n",
        "        # Context enhancement parameters\n",
        "        \"context_enhancers\": [\n",
        "            [],  # No enhancement\n",
        "            [\"summarize\"],\n",
        "            [\"reorder\"],\n",
        "            [\"highlight\"]\n",
        "            #[\"deduplicate\"],\n",
        "            #[\"reorder\", \"deduplicate\"],\n",
        "            #[\"summarize\", \"highlight\"]\n",
        "        ],\n",
        "\n",
        "        # Generation parameters\n",
        "        #\"prompt_strategies\": [\"standard\", \"cot\", \"few_shot\"],\n",
        "        \"prompt_strategies\": [\"standard\"],\n",
        "       # \"models\": [\"qwen\", \"flan-t5\", \"tiny-llama\", \"phi-2\"]\n",
        "        \"models\": [\"qwen\", \"flan-t5\"]\n",
        "    }\n",
        "\n",
        "    # Simplified configuration for faster testing\n",
        "    FAST_CONFIG = {\n",
        "        \"test_queries\": [\n",
        "            \"What is the policy for academic probation?\",\n",
        "            \"What are the requirements for graduation?\"\n",
        "        ],\n",
        "        \"chunk_sizes\": [512],\n",
        "        \"chunk_overlaps\": [64],\n",
        "        \"chunking_strategies\": [\"recursive\", \"sentence\"],\n",
        "        \"retrieval_methods\": [\"dense\", \"hybrid\"],\n",
        "        \"reranking_options\": [False],\n",
        "        \"top_k_values\": [5],\n",
        "        \"context_enhancers\": [[], [\"summarize\"]],\n",
        "        \"prompt_strategies\": [\"standard\"],\n",
        "        \"models\": [\"qwen\", \"flan-t5\"]\n",
        "    }\n",
        "\n",
        "    # Choose which configuration to use\n",
        "    # EXPERIMENT_CONFIG = COMPREHENSIVE_CONFIG  # Uncomment for full experiment\n",
        "    EXPERIMENT_CONFIG = FAST_CONFIG           # Use for fast testing\n",
        "\n",
        "\n",
        "    # Initialize pipeline with multiple PDFs\n",
        "    print(f\"\\nInitializing RAG pipeline with PDFs: {PDF_FILES}\")\n",
        "    pipeline = AdvancedRagPipeline(PDF_FILES)\n",
        "\n",
        "    # Run full experiment\n",
        "    print(\"\\nStarting RAG experiment with configuration:\")\n",
        "    for key, value in EXPERIMENT_CONFIG.items():\n",
        "        print(f\"- {key}: {value if not isinstance(value, list) else len(value)} options\")\n",
        "\n",
        "    results_df = pipeline.experiment(EXPERIMENT_CONFIG)\n",
        "\n",
        "    # Run interactive demo with best configurations\n",
        "    best_config = results_df.loc[results_df['composite_score'].idxmax()]\n",
        "    print(f\"\\nStarting interactive demo with best configuration:\")\n",
        "    print(f\"- Retrieval: {best_config['retrieval_method']}\")\n",
        "    print(f\"- Model: {best_config['model']}\")\n",
        "    print(f\"- Top K: {best_config['top_k']}\")\n",
        "\n",
        "    pipeline.prepare_for_demo()\n",
        "    run_interactive_demo(\n",
        "        pipeline,\n",
        "        retrieval_method=best_config['retrieval_method'],\n",
        "        model_type=best_config['model'],\n",
        "        top_k=int(best_config['top_k'])\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"best_rag_config.json\", \"w\") as f:\n",
        "    json.dump(best_config.to_dict(), f, indent=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "M-m2HC5n47s8",
        "outputId": "2f2c13a2-fa25-4f08-8f71-ded94f063af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'to_dict'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c5843bb957e3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best_rag_config.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to_dict'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "faiss.write_index(pipeline.retriever.index_ip, \"rag_faiss_index.bin\")"
      ],
      "metadata": {
        "id": "_m4KPHPK5DHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best configuration\n",
        "import json\n",
        "with open(\"best_rag_config.json\", \"r\") as f:\n",
        "    best_config = json.load(f)\n",
        "\n",
        "# Initialize pipeline with two PDF files\n",
        "PDF_FILES = [\n",
        "    \"/content/student-handbook2022-23.pdf\",\n",
        "    \"/content/parking-regulations.pdf\"  # Replace with your second PDF path\n",
        "]\n",
        "pipeline = AdvancedRagPipeline(PDF_FILES)  # Pass list of PDF paths\n",
        "data_processor = DataProcessor()\n",
        "\n",
        "# Load and chunk both PDFs\n",
        "chunks = data_processor.load_and_chunk(\n",
        "    pipeline.file_paths,\n",
        "    chunking_strategy=best_config['chunking_strategy'],\n",
        "    chunk_size=best_config['chunk_size'],\n",
        "    chunk_overlap=best_config['chunk_overlap']\n",
        ")\n",
        "\n",
        "# Create retriever with combined chunks\n",
        "pipeline.retriever = AdvancedRetriever(chunks)\n",
        "\n",
        "# Load the saved FAISS index (ensure it was created with both PDFs)\n",
        "try:\n",
        "    pipeline.retriever.index_ip = faiss.read_index(\"rag_faiss_index.bin\")\n",
        "except FileNotFoundError:\n",
        "    st.error(\"FAISS index not found. Please run the experiment first to generate the index.\")\n",
        "    st.stop()\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"RAG System Demo\")\n",
        "query = st.text_input(\"Ask a question about the student handbook or parking regulations:\")\n",
        "if st.button(\"Submit\"):\n",
        "    contexts, _ = pipeline.retriever.search(\n",
        "        query=query,\n",
        "        k=int(best_config['top_k']),\n",
        "        method=best_config['retrieval_method'],\n",
        "        rerank=best_config['reranking'] == \"Yes\"\n",
        "    )\n",
        "    context_str = \"\\n\\n\".join([c.page_content for c in contexts])\n",
        "    if best_config['context_enhancement'] != \"None\":\n",
        "        context_str, _ = pipeline.context_enhancer.enhance_context(\n",
        "            contexts, query, best_config['context_enhancement'].split(\"+\")\n",
        "        )\n",
        "    answer, _ = pipeline._generate_answer(\n",
        "        query=query,\n",
        "        context=context_str,\n",
        "        model_type=best_config['model'],\n",
        "        prompt_strategy=best_config['prompt_strategy']\n",
        "    )\n",
        "    st.write(\"**Answer:**\")\n",
        "    st.write(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "1sASa0-N5uyg",
        "outputId": "b3d7aea6-3a07-481a-eada-2031a40dacad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 1 column 1 (char 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-c9343eaf40a0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load best configuration and index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best_rag_config.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mbest_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdvancedRagPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/student-handbook2022-23.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reinitialize with your PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdata_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Initialize DataProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar -xvf ngrok-v3-stable-linux-amd64.tgz\n",
        "!mv ngrok /usr/local/bin/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1AMVCK6_RZ2",
        "outputId": "519207b9-9ad8-4445-9587-83ffd56c6a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-11 22:34:09--  https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 99.83.220.108, 75.2.60.68, 35.71.179.82, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|99.83.220.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9395172 (9.0M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-v3-stable-linux-amd64.tgz’\n",
            "\n",
            "ngrok-v3-stable-lin 100%[===================>]   8.96M  13.3MB/s    in 0.7s    \n",
            "\n",
            "2025-04-11 22:34:13 (13.3 MB/s) - ‘ngrok-v3-stable-linux-amd64.tgz’ saved [9395172/9395172]\n",
            "\n",
            "ngrok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ngrok and pyngrok\n",
        "!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz -q\n",
        "!tar -xvf ngrok-v3-stable-linux-amd64.tgz -q\n",
        "!mv ngrok /usr/local/bin/\n",
        "!pip install pyngrok -q\n",
        "\n",
        "# Authenticate ngrok\n",
        "!ngrok authtoken 2vbPj6xVgq76zpNz5SyudZB8Ymg_5zpSqYBXLt1Ekj83wZSGx\n",
        "\n",
        "# Run Streamlit and expose it\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "ngrok.kill()  # Clear previous tunnels\n",
        "process = subprocess.Popen(['streamlit', 'run', '/content/app.py', '--server.port', '8501'])\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Access your Streamlit app at: {public_url}\")\n",
        "\n",
        "import time\n",
        "time.sleep(3600)  # Keep running for 1 hour"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-CGgHofGvcl",
        "outputId": "eae1d201-d52c-4a61-e57c-eab5629356a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: invalid option -- 'q'\n",
            "Try 'tar --help' or 'tar --usage' for more information.\n",
            "mv: cannot stat 'ngrok': No such file or directory\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Access your Streamlit app at: NgrokTunnel: \"https://05ee-34-83-151-40.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2vbPj6xVgq76zpNz5SyudZB8Ymg_5zpSqYBXLt1Ekj83wZSGx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPYi764D_y7m",
        "outputId": "4b540151-b604-462a-978d-d68fe0a1efbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "!pip install pyngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Start Streamlit in the background\n",
        "process = subprocess.Popen(['streamlit', 'run', 'app.py', '--server.port', '8501'])\n",
        "\n",
        "# Create a public URL with ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Access your Streamlit app at: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujWprWJM_6I0",
        "outputId": "3cde27b4-5d8e-4be6-86fb-3cf10c1c8001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Access your Streamlit app at: NgrokTunnel: \"https://3de3-34-83-151-40.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}